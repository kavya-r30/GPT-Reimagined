Summary

This video explores gradient descent and optimization in neural networks, emphasizing optimal step sizes and various optimization algorithms.

1. Gradient Descent: Key method for finding the optimum in neural networks.
2. Optimal Step Size: Finding this can yield a solution in one step.
![alt text](images\6_image.png)
3. Overshooting: Too large a step size leads to overshooting the optimum.
4. Bouncing Back: Doubling the optimal step size can cause oscillation.
5. Exponential Decay: 
![alt text](images\6_image-1.png)
6. Newtonâ€™s Method: Effective for quadratic functions but challenging in multiple dimensions.
7. RProp: 
- When the signs are the same, we go in the same direction as in the previous iteration. Since this seems to be a good direction, the step size should be increased to go to the optimum more quickly
- If the sign changed, the new update is moving in a different direction. This means that we just jumped over an optimum. The step size should be decreased to avoid jumping over the optimum again
8. QuickProp: Assumes the error surface can be approximated as a quadratic function and finds convergence by using a second-order approximation of the error function.
9. Momentum Technique: Adjusts step sizes based on gradient direction by maintaining running averages for convergence.
- first computes the gradient step then adds scaled previous step(running average).
10. Nestrov's Accelrated gradient: first adds the previous step (running average) then compute the gradient step and add to obtain final step.