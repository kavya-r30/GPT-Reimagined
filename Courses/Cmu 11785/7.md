Summary

Stochastic Gradient Descent (SGD) processes data incrementally, offering faster convergence but risking poorer minima due to variance. Optimization techniques like Adam improve performance.

1. SGD Efficiency: Stochastic Gradient Descent processes data incrementally, enhancing convergence speed over mini batch but the with higher loss.
2. Oscillation Risks: If input order isnâ€™t randomized, SGD can exhibit oscillatory behavior during training.
3. Mini-batch: Using mini-batch updates reduces variance in loss calculations, allowing for more stable gradients and quicker convergence in training.
3. Step Size Importance: Convergence to the global optimum requires decreasing step sizes over time.
4. Empirical vs. True Error: The empirical error calculated from sample points may not reflect the true expected error.
5. Variance Reduction: Mini-batch updates can help decrease variance in loss function estimates.
6. Momentum-based updates smooths out the gradient and RMSprop is crucial for adjusting learning rates effectively.
7. Adam Optimization: The Adam algorithm combines momentum and RMSprop techniques for improved optimization performance.
![alt text](images\7_image.png)