Lecture on Neural Networks Representation

Introduction
Purpose: Explore what neural networks can represent and their limitations.
Key Factors: Depth, width, and activations.
Recap: Tasks like speech recognition, playing games, and translation achieved by neural networks.
Neural Networks Basics
Neurons: Mimic human brain structure, composed of many inputs and outputs.
Perceptron: Basic unit that computes a weighted sum of inputs; outputs 1 if sum exceeds threshold, otherwise 0.
Activation Functions: Various types including threshold, sigmoid, tanh, rectification, and softplus.

Perceptrons and Boolean Functions
Perceptron Capabilities: Can model simple Boolean functions like AND, NOT, and OR gates.
Limitations: Not capable of computing XOR function independently, networks help to overcome this.
MLPs as Universal Boolean Functions: Can model any Boolean function, given enough layers and neurons.

Multi-Layer Perceptrons (MLPs)
MLP: Network of perceptrons arranged layer-wise.
Depth: Defined by the longest path from source (input) to sink (output).
Layers: Grouped by neurons at the same depth with respect to input.
Deep Network: Greater depth leads to potentially fewer neurons required.

"A single hidden layer requires 2^(n-1) maxium number of neuron of n inputs"
"Same will require 3(n-1) layers in deep network"


MLPs for Boolean Functions
1 Hidden Layer Sufficiency: A single hidden layer can model any Boolean function but can require exponentially many neurons.
Depth Efficiency: Deeper networks reduce the required neurons from exponentially many to linear in some cases.
Classification and Decision Boundaries

Basic Classification
Perceptrons as Classifiers: Create decision boundaries for classification tasks.
Multi-Layer Networks: Combine multiple perceptrons for complex decision boundaries (e.g., pentagons, complex shapes).

Depth vs. Width
Importance of Sufficient Width: Every layer in the network must be wide enough to capture essential information, otherwise it can't be recovered in subsequent layers.
Activation Functions: Sigmoid and relu activations allow networks to maintain information about input distances from decision boundaries.
Deep Networks: More expressive than shallow networks.

Continuous-Valued Regression
Function Approximation: Even single hidden layer networks can approximate continuous functions to arbitrary precision using pulse functions.
Higher Dimensions: More complex constructions involving cylinders and sums for approximation.
Universal Approximator: Single hidden layer networks can approximate any continuous function, but may need infinite neurons for perfect approximation.

Summary and Recap
MLPs are Universal: Can model any function (Boolean, classification, regression) with sufficient depth and width.
Deeper Networks Efficiency: Achieve same precision with far fewer neurons compared to shallow networks.
Proper Activations Critical: Activation functions play a crucial role in passing information through the layers.

Final Remarks
Hidden Slides: Additional slides are in the slide deck and will feature in weekend quizzes.
Error Tolerance: One hidden layer networks need infinite neurons for zero error, deeper networks drastically reduce neuron count.

Practical Considerations
General Trends: Network depth and proper activation functions can significantly optimize neural network performance.