Summary

Batch normalization, weight decay, dropouts are essential for effective neural network training, improving stability and preventing overfitting.

1. Batch Normalization: Normalizing mini batches facilitates effective learning by stabilizing the training process and reducing internal covariate shift, leading to faster convergence. It also ensures mini batches have zero mean and unit variance.
![alt text](images\8_image_3.png)
- We do batch normalization bcoz every minibatches are diff from each other and this differences scale up and batches are spread far in different space so we we shift all the batches to the origin and scale them a specific location to mitigate this
2. Backpropagation: The normalization process introduces dependencies among mini batch instances, which requires careful handling of parameters like gamma and beta.
![alt text](images\8_image_4.png)
![alt text](images\8_image_5.png)
2. Diverse Mini-Batches: If all the mini batches are identical or similar batch normalization blocks the back prop as the gradient becomes zero so diverse mini-batches helps mitigate covariate shift, enabling the network to learn more generalized features from the data.
2. Inference: During testing we need the value of mean and std deviation therefore we pass the whole training batch data to calculate its mean and std deviation or we can maintain a running mean and std deviation with momentum which essentially gives us indentical or similar value of mean and std dev, which can be done during training.
![alt text](images\8_image_6.png)
2. Error Minimization vs. Weight Control: Striking a balance between minimizing prediction errors and controlling weight magnitudes is essential for developing models that generalize well.
2. Weight Decay: Constraining the weights w to be close to zero will force slower perceptron and smoother output and also it prevents the neurons for becoming dead so that so the weights can update and participate.
2. Regularization Terms: Incorporating regularization terms into the loss function for preventing large weights, thus promoting simpler models that generalize better. Regularization terms in the loss function helps penalize large weights thus leading to more stable networks.
![alt text](images\8_image_7.png)
2. Dropout: By randomly turning off neurons during training, dropout creates a diverse ensemble of models, enhancing generalization of the final model and preventing overfitting.
2. Gradient Clipping: Implementing gradient clipping protects against sudden changes in model parameters, stabilizing the training process and preventing divergence.