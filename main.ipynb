{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aabid',\n",
       " 'aabida',\n",
       " 'aachal',\n",
       " 'aadesh',\n",
       " 'aadil',\n",
       " 'aadish',\n",
       " 'aaditya',\n",
       " 'aaenab',\n",
       " 'aafreen',\n",
       " 'aafrin',\n",
       " 'aaftaab',\n",
       " 'aaftab',\n",
       " 'aagand',\n",
       " 'aahim',\n",
       " 'aajad',\n",
       " 'aajiv',\n",
       " 'aakanksha',\n",
       " 'aakar',\n",
       " 'aakas',\n",
       " 'aakash',\n",
       " 'aakib',\n",
       " 'aalam',\n",
       " 'aalina',\n",
       " 'aaliya',\n",
       " 'aamil',\n",
       " 'aamin',\n",
       " 'aamina',\n",
       " 'aamir',\n",
       " 'aamod',\n",
       " 'aamosh',\n",
       " 'aamrin',\n",
       " 'aanad',\n",
       " 'aanamika',\n",
       " 'aanand',\n",
       " 'aanchal',\n",
       " 'aanik',\n",
       " 'aanil',\n",
       " 'aansi',\n",
       " 'aansu',\n",
       " 'aanya',\n",
       " 'aaradhana',\n",
       " 'aarati',\n",
       " 'aarav',\n",
       " 'aardhna',\n",
       " 'aarif',\n",
       " 'aarifa',\n",
       " 'aarifun',\n",
       " 'aarju',\n",
       " 'aarti',\n",
       " 'aarushi',\n",
       " 'aas',\n",
       " 'aasa',\n",
       " 'aash',\n",
       " 'aasha',\n",
       " 'aashi',\n",
       " 'aashia',\n",
       " 'aashif',\n",
       " 'aashik',\n",
       " 'aashis',\n",
       " 'aashish',\n",
       " 'aashiya',\n",
       " 'aashma',\n",
       " 'aashu',\n",
       " 'aasif',\n",
       " 'aasim',\n",
       " 'aasish',\n",
       " 'aasma',\n",
       " 'aasmin',\n",
       " 'aastha',\n",
       " 'aasto',\n",
       " 'aasu',\n",
       " 'aatam',\n",
       " 'aatif',\n",
       " 'aatikun',\n",
       " 'aatir',\n",
       " 'aavesh',\n",
       " 'aayana',\n",
       " 'aayesha',\n",
       " 'aaysha',\n",
       " 'aayush',\n",
       " 'aazad',\n",
       " 'aazadi',\n",
       " 'abash',\n",
       " 'abbal',\n",
       " 'abbas',\n",
       " 'abdul',\n",
       " 'abdulla',\n",
       " 'abdullah',\n",
       " 'abha',\n",
       " 'abhaki',\n",
       " 'abhash',\n",
       " 'abhay',\n",
       " 'abhaysingh',\n",
       " 'abhi',\n",
       " 'abhijeet',\n",
       " 'abhijit',\n",
       " 'abhilash',\n",
       " 'abhilasha',\n",
       " 'abhimanu',\n",
       " 'abhimanyu',\n",
       " 'abhinandan',\n",
       " 'abhinash',\n",
       " 'abhinav',\n",
       " 'abhinay',\n",
       " 'abhinwav',\n",
       " 'abhiraj',\n",
       " 'abhisek',\n",
       " 'abhisekh',\n",
       " 'abhishak',\n",
       " 'abhishek',\n",
       " 'abhishekh',\n",
       " 'abhishiek',\n",
       " 'abhishik',\n",
       " 'abid',\n",
       " 'abida',\n",
       " 'abishak',\n",
       " 'abishek',\n",
       " 'abrar',\n",
       " 'abu',\n",
       " 'abul',\n",
       " 'achin',\n",
       " 'adalat',\n",
       " 'adan',\n",
       " 'adarsh',\n",
       " 'adersen',\n",
       " 'adesh',\n",
       " 'adhish',\n",
       " 'adi',\n",
       " 'adiba',\n",
       " 'adil',\n",
       " 'adisan',\n",
       " 'aditi',\n",
       " 'aditiya',\n",
       " 'aditya',\n",
       " 'adityalok',\n",
       " 'adnan',\n",
       " 'adrash',\n",
       " 'aejaz',\n",
       " 'aesha',\n",
       " 'afarin',\n",
       " 'affan',\n",
       " 'afjal',\n",
       " 'afridi',\n",
       " 'afrin',\n",
       " 'afrina',\n",
       " 'afroz',\n",
       " 'afsana',\n",
       " 'afsar',\n",
       " 'afsari',\n",
       " 'afsha',\n",
       " 'afshana',\n",
       " 'afshar',\n",
       " 'aftab',\n",
       " 'afzaj',\n",
       " 'afzal',\n",
       " 'agrej',\n",
       " 'agyapad',\n",
       " 'ahamad',\n",
       " 'ahmad',\n",
       " 'ahmed',\n",
       " 'ahsamin',\n",
       " 'ahsan',\n",
       " 'ahupendra',\n",
       " 'aidtya',\n",
       " 'aijay',\n",
       " 'aisha',\n",
       " 'aishe',\n",
       " 'aishwarya',\n",
       " 'ajab',\n",
       " 'ajad',\n",
       " 'ajahar',\n",
       " 'ajaj',\n",
       " 'ajara',\n",
       " 'ajay',\n",
       " 'ajaypal',\n",
       " 'ajeem',\n",
       " 'ajeet',\n",
       " 'ajim',\n",
       " 'ajima',\n",
       " 'ajit',\n",
       " 'ajju',\n",
       " 'ajkati',\n",
       " 'ajmal',\n",
       " 'ajman',\n",
       " 'ajmer',\n",
       " 'ajmeri',\n",
       " 'ajmit',\n",
       " 'ajnabi',\n",
       " 'ajnoor',\n",
       " 'ajra',\n",
       " 'akaash',\n",
       " 'akansh',\n",
       " 'akansha',\n",
       " 'akash',\n",
       " 'akashya',\n",
       " 'akbar',\n",
       " 'akbari',\n",
       " 'akeel',\n",
       " 'akhalak',\n",
       " 'akhatari',\n",
       " 'akheelesh',\n",
       " 'akhil',\n",
       " 'akhilesh',\n",
       " 'akhileshwer',\n",
       " 'akhlaq',\n",
       " 'akhlesh',\n",
       " 'akhlish',\n",
       " 'akhtar',\n",
       " 'akhtari',\n",
       " 'akib',\n",
       " 'akif',\n",
       " 'akil',\n",
       " 'akilesh',\n",
       " 'akkash',\n",
       " 'akkni',\n",
       " 'akram',\n",
       " 'akshat',\n",
       " 'akshay',\n",
       " 'akshit',\n",
       " 'akshita',\n",
       " 'akshpaat',\n",
       " 'akthari',\n",
       " 'aktri',\n",
       " 'al0k',\n",
       " 'alahbasri',\n",
       " 'alam',\n",
       " 'alapna',\n",
       " 'alaudin',\n",
       " 'albaksha',\n",
       " 'albina',\n",
       " 'aleesha',\n",
       " 'alema',\n",
       " 'alhamdi',\n",
       " 'ali',\n",
       " 'alijan',\n",
       " 'alim',\n",
       " 'alima',\n",
       " 'alina',\n",
       " 'alis',\n",
       " 'alish',\n",
       " 'alisha',\n",
       " 'alita',\n",
       " 'aliya',\n",
       " 'alka',\n",
       " 'alkesh',\n",
       " 'allabaksh',\n",
       " 'allaraji',\n",
       " 'allauddin',\n",
       " 'alma',\n",
       " 'almina',\n",
       " 'alok',\n",
       " 'alseepa',\n",
       " 'altab',\n",
       " 'altaf',\n",
       " 'altmash',\n",
       " 'aluddin',\n",
       " 'aman',\n",
       " 'amana',\n",
       " 'amandeep',\n",
       " 'amanjeet',\n",
       " 'amar',\n",
       " 'amarapal',\n",
       " 'amarchand',\n",
       " 'amardeep',\n",
       " 'amarendra',\n",
       " 'amarin',\n",
       " 'amarjeet',\n",
       " 'amarjit',\n",
       " 'amarmula',\n",
       " 'amarnath',\n",
       " 'amart',\n",
       " 'amarvesh',\n",
       " 'amasi',\n",
       " 'amba',\n",
       " 'amber',\n",
       " 'ambika',\n",
       " 'ambiya',\n",
       " 'ameen',\n",
       " 'ameer',\n",
       " 'ameera',\n",
       " 'amena',\n",
       " 'amida',\n",
       " 'amie',\n",
       " 'amijan',\n",
       " 'amil',\n",
       " 'amin',\n",
       " 'amina',\n",
       " 'amir',\n",
       " 'amiraka',\n",
       " 'amiri',\n",
       " 'amirul',\n",
       " 'amisha',\n",
       " 'amit',\n",
       " '\"amit',\n",
       " 'amita',\n",
       " 'amjad',\n",
       " 'amjat',\n",
       " 'amkit',\n",
       " 'amlesh',\n",
       " 'ammar',\n",
       " 'amna',\n",
       " 'amol',\n",
       " 'amot',\n",
       " 'amrat',\n",
       " 'amratalal',\n",
       " 'amreek',\n",
       " 'amreen',\n",
       " 'amreeta',\n",
       " 'amri',\n",
       " 'amrik',\n",
       " 'amrika',\n",
       " 'amrin',\n",
       " 'amrish',\n",
       " 'amrit',\n",
       " 'amrita',\n",
       " 'amritha',\n",
       " 'amritpal',\n",
       " 'amrjeet',\n",
       " 'amrooti',\n",
       " 'amrta',\n",
       " 'amrudin',\n",
       " 'amuchla',\n",
       " 'amuda',\n",
       " 'amzad',\n",
       " 'anadi',\n",
       " 'anaji',\n",
       " 'anam',\n",
       " 'anamika',\n",
       " 'anamol',\n",
       " 'anand',\n",
       " 'anandi',\n",
       " 'anandu',\n",
       " 'anandwati',\n",
       " 'anantram',\n",
       " 'anara',\n",
       " 'anarkali',\n",
       " 'anaro',\n",
       " 'anash',\n",
       " 'anayatha',\n",
       " 'anchal',\n",
       " 'andhav',\n",
       " 'aneeta',\n",
       " 'anekha',\n",
       " 'angad',\n",
       " 'angan',\n",
       " 'angda',\n",
       " 'angreg',\n",
       " 'anguri',\n",
       " 'anikesh',\n",
       " 'aniket',\n",
       " 'anikt',\n",
       " 'anil',\n",
       " 'animesh',\n",
       " 'anirudh',\n",
       " 'anish',\n",
       " 'anisha',\n",
       " 'anita',\n",
       " 'anja',\n",
       " 'anjal',\n",
       " 'anjali',\n",
       " 'anjaly',\n",
       " 'anjan',\n",
       " 'anjana',\n",
       " 'anjani',\n",
       " 'anjara',\n",
       " 'anjeev',\n",
       " 'anjila',\n",
       " 'anjli',\n",
       " 'anjna',\n",
       " 'anjrej',\n",
       " 'anju',\n",
       " 'anjum',\n",
       " 'anjuman',\n",
       " 'ankaj',\n",
       " 'ankala',\n",
       " 'ankeet',\n",
       " 'ankesh',\n",
       " 'ankit',\n",
       " 'ankita',\n",
       " 'ankur',\n",
       " 'ankus',\n",
       " 'ankush',\n",
       " 'anli',\n",
       " 'anmol',\n",
       " 'annat',\n",
       " 'annavi',\n",
       " 'annielal',\n",
       " 'anno',\n",
       " 'annu',\n",
       " 'anoj',\n",
       " 'anokha',\n",
       " 'anooj',\n",
       " 'anoop',\n",
       " 'anoopama',\n",
       " 'anoura',\n",
       " 'ansal',\n",
       " 'ansar',\n",
       " 'ansh',\n",
       " 'anshika',\n",
       " 'anshiya',\n",
       " 'anshu',\n",
       " 'anshul',\n",
       " 'anshum',\n",
       " 'anshuman',\n",
       " 'ansu',\n",
       " 'ansul',\n",
       " 'anta',\n",
       " 'antim',\n",
       " 'antima',\n",
       " 'anu',\n",
       " 'anubhav',\n",
       " 'anuj',\n",
       " 'anuja',\n",
       " 'anup',\n",
       " 'anupam',\n",
       " 'anupama',\n",
       " 'anupuma',\n",
       " 'anuradha',\n",
       " 'anurag',\n",
       " 'anuraj',\n",
       " 'anurudh',\n",
       " 'anushka',\n",
       " 'anuska',\n",
       " 'anusoya',\n",
       " 'anuu',\n",
       " 'anwar',\n",
       " 'anwari',\n",
       " 'any',\n",
       " 'anzum',\n",
       " 'aoosaf',\n",
       " 'aparna',\n",
       " 'aphasana',\n",
       " 'aphsana',\n",
       " 'apsana',\n",
       " 'arab',\n",
       " 'araddhna',\n",
       " 'aradhana',\n",
       " 'arajun',\n",
       " 'araslan',\n",
       " 'arati',\n",
       " 'aravind',\n",
       " 'arbaj',\n",
       " 'arbana',\n",
       " 'arbaz',\n",
       " 'archan',\n",
       " 'archana',\n",
       " 'archita',\n",
       " 'archna',\n",
       " 'areen',\n",
       " 'arham',\n",
       " 'arhiyant',\n",
       " 'ariba',\n",
       " 'arif',\n",
       " 'arifa',\n",
       " 'arindom',\n",
       " 'arindra',\n",
       " 'arjina',\n",
       " 'arjun',\n",
       " 'armaan',\n",
       " 'arman',\n",
       " 'armita',\n",
       " 'arnab',\n",
       " 'arnabjit',\n",
       " 'arpit',\n",
       " 'arpna',\n",
       " 'arsad',\n",
       " 'arsh',\n",
       " 'arshad',\n",
       " 'arshi',\n",
       " 'arshla',\n",
       " 'arsi',\n",
       " 'arthi',\n",
       " 'arti',\n",
       " 'aru',\n",
       " 'arun',\n",
       " 'aruna',\n",
       " 'aruni',\n",
       " 'arunkumar',\n",
       " 'arvind',\n",
       " 'arvinder',\n",
       " 'aryan',\n",
       " 'arzoo',\n",
       " 'asagar',\n",
       " 'asana',\n",
       " 'asanti',\n",
       " 'asarfi',\n",
       " 'asfak',\n",
       " 'asfaq',\n",
       " 'asgar',\n",
       " 'asgari',\n",
       " 'asha',\n",
       " 'ashad',\n",
       " 'ashak',\n",
       " 'ashanjali',\n",
       " 'asharam',\n",
       " 'asharani',\n",
       " 'ashfaq',\n",
       " 'ashif',\n",
       " 'ashik',\n",
       " 'ashima',\n",
       " 'ashiqu',\n",
       " 'ashish',\n",
       " 'ashishi',\n",
       " 'ashiya',\n",
       " 'ashlam',\n",
       " 'ashma',\n",
       " 'ashman',\n",
       " 'ashmi',\n",
       " 'ashmin',\n",
       " 'ashnu',\n",
       " 'asho',\n",
       " 'ashok',\n",
       " 'ashra',\n",
       " 'ashrabi',\n",
       " 'ashraf',\n",
       " 'ashrfi',\n",
       " 'ashshwer',\n",
       " 'ashsish',\n",
       " 'ashtha',\n",
       " 'ashu',\n",
       " 'ashutosh',\n",
       " 'ashwani',\n",
       " 'ashwini',\n",
       " 'asi',\n",
       " 'asif',\n",
       " 'asish',\n",
       " 'asiwani',\n",
       " 'asiya',\n",
       " 'aslam',\n",
       " 'asma',\n",
       " 'asman',\n",
       " 'asmat',\n",
       " 'asmin',\n",
       " 'asmina',\n",
       " 'asmit',\n",
       " 'asmita',\n",
       " 'asrani',\n",
       " 'asrf',\n",
       " 'asruddin',\n",
       " 'astha',\n",
       " 'asutosh',\n",
       " 'aswani',\n",
       " 'aswin',\n",
       " 'atanu',\n",
       " 'ateek',\n",
       " 'atif',\n",
       " 'atiq',\n",
       " 'atish',\n",
       " 'atitaj',\n",
       " 'atta',\n",
       " 'attar',\n",
       " 'atu',\n",
       " 'atul',\n",
       " 'avadh',\n",
       " 'avantika',\n",
       " 'avastha',\n",
       " 'avdesh',\n",
       " 'avdhesh',\n",
       " 'avfesh',\n",
       " 'avi',\n",
       " 'avid',\n",
       " 'avinas',\n",
       " 'avinash',\n",
       " 'avinsh',\n",
       " 'avneet',\n",
       " 'avnish',\n",
       " 'avnit',\n",
       " 'avtar',\n",
       " 'awdesh',\n",
       " 'awdhesh',\n",
       " 'awedhesh',\n",
       " 'awshin',\n",
       " 'axat',\n",
       " 'ayan',\n",
       " 'ayasha',\n",
       " 'aysha',\n",
       " 'ayub',\n",
       " 'ayube',\n",
       " 'ayush',\n",
       " 'ayushi',\n",
       " 'azad',\n",
       " 'azam',\n",
       " 'azaruddin',\n",
       " 'azaz',\n",
       " 'azhar',\n",
       " 'azim',\n",
       " 'azmira',\n",
       " 'azruddin',\n",
       " 'baam',\n",
       " 'babali',\n",
       " 'babalu',\n",
       " 'baban',\n",
       " 'babar',\n",
       " 'babbi',\n",
       " 'babbu',\n",
       " 'babby',\n",
       " 'babita',\n",
       " 'babli',\n",
       " 'babloo',\n",
       " 'bablu',\n",
       " 'babram',\n",
       " 'babu',\n",
       " 'babudden',\n",
       " 'babuddin',\n",
       " 'babul',\n",
       " 'babulal',\n",
       " 'babupuri',\n",
       " 'baburam',\n",
       " 'baby',\n",
       " 'baccha',\n",
       " 'bacche',\n",
       " 'bacchu',\n",
       " 'bachan',\n",
       " 'bachcha',\n",
       " 'bachheshwar',\n",
       " 'bachhu',\n",
       " 'bachu',\n",
       " 'bada',\n",
       " 'badal',\n",
       " 'badami',\n",
       " 'badan',\n",
       " 'badarjahan',\n",
       " 'badnath',\n",
       " 'badrealam',\n",
       " 'badri',\n",
       " 'badru',\n",
       " 'badrudeen',\n",
       " 'badrulla',\n",
       " 'badrunisha',\n",
       " 'badshya',\n",
       " 'baga',\n",
       " 'bagaram',\n",
       " 'bagdai',\n",
       " 'bagga',\n",
       " 'baggusingh',\n",
       " 'bagwan',\n",
       " 'bahadur',\n",
       " 'bahalen',\n",
       " 'bahart',\n",
       " 'bahnu',\n",
       " 'bahrat',\n",
       " 'bahwana',\n",
       " 'baichan',\n",
       " 'baidnath',\n",
       " 'baijnath',\n",
       " 'baita',\n",
       " 'bajinder',\n",
       " 'bajrang',\n",
       " 'bajrangi',\n",
       " 'bajulal',\n",
       " 'bake',\n",
       " 'bakhtvaar',\n",
       " 'bakila',\n",
       " 'baksi',\n",
       " 'bal',\n",
       " 'bala',\n",
       " 'balak',\n",
       " 'balalia',\n",
       " 'balam',\n",
       " 'balaram',\n",
       " 'balbeer',\n",
       " 'balbir',\n",
       " 'balbiri',\n",
       " 'balchand',\n",
       " 'baldev',\n",
       " 'baleshwar',\n",
       " 'baleshwer',\n",
       " 'bali',\n",
       " 'baliram',\n",
       " 'baliya',\n",
       " 'baljeet',\n",
       " 'baljinder',\n",
       " 'balkishan',\n",
       " 'ballu',\n",
       " 'balmiki',\n",
       " 'balraj',\n",
       " 'balram',\n",
       " 'balveer',\n",
       " 'balvinder',\n",
       " 'balwan',\n",
       " 'balwant',\n",
       " 'banarsi',\n",
       " 'bandana',\n",
       " 'bandani',\n",
       " 'bandu',\n",
       " 'bangali',\n",
       " 'banita',\n",
       " 'baniya',\n",
       " 'banju',\n",
       " 'bannu',\n",
       " 'banshi',\n",
       " 'bansi',\n",
       " 'bansilal',\n",
       " 'banti',\n",
       " 'bantu',\n",
       " 'banty',\n",
       " 'banu',\n",
       " 'banwari',\n",
       " 'banwarilal',\n",
       " 'bapu',\n",
       " 'barakha',\n",
       " 'barham',\n",
       " 'barj',\n",
       " 'barjesh',\n",
       " 'barjraj',\n",
       " 'barkat',\n",
       " 'barkha',\n",
       " 'barti',\n",
       " 'basant',\n",
       " 'basanta',\n",
       " 'basanti',\n",
       " 'bashanti',\n",
       " 'bashudev',\n",
       " 'basibul',\n",
       " 'basiran',\n",
       " 'basnti',\n",
       " 'batasi',\n",
       " 'batloon',\n",
       " 'bato',\n",
       " 'batu',\n",
       " 'batul',\n",
       " 'baudi',\n",
       " 'bavita',\n",
       " 'bawan',\n",
       " 'beauty',\n",
       " 'bebi',\n",
       " 'beby',\n",
       " 'bechan',\n",
       " 'beekar',\n",
       " 'beena',\n",
       " 'beenu',\n",
       " 'beera',\n",
       " 'beeram',\n",
       " 'beeru',\n",
       " 'beeti',\n",
       " 'beeuty',\n",
       " 'begam',\n",
       " 'begraj',\n",
       " 'begum',\n",
       " 'begumpur',\n",
       " 'belo',\n",
       " 'benay',\n",
       " 'beni',\n",
       " 'benjir',\n",
       " 'benu',\n",
       " 'beragi',\n",
       " 'beti',\n",
       " 'bhabhav',\n",
       " 'bhabhiya',\n",
       " 'bhaddal',\n",
       " 'bhadur',\n",
       " 'bhag',\n",
       " 'bhagat',\n",
       " 'bhagay',\n",
       " 'bhagchand',\n",
       " 'bhaggo',\n",
       " 'bhagipuri',\n",
       " 'bhagirath',\n",
       " 'bhagirathi',\n",
       " 'bhagvaan',\n",
       " 'bhagvati',\n",
       " 'bhagwan',\n",
       " 'bhagwana',\n",
       " 'bhagwanaram',\n",
       " 'bhagwanti',\n",
       " 'bhagwat',\n",
       " 'bhagwati',\n",
       " 'bhagya',\n",
       " 'bhahadur',\n",
       " 'bhajan',\n",
       " 'bhajandeep',\n",
       " 'bhajanlal',\n",
       " 'bhajju',\n",
       " 'bhakuni',\n",
       " 'bhala',\n",
       " 'bhalaram',\n",
       " 'bhan',\n",
       " 'bhani',\n",
       " 'bhanmati',\n",
       " 'bhanu',\n",
       " 'bhanumati',\n",
       " 'bhanupriya',\n",
       " 'bhanvar',\n",
       " 'bhanwar',\n",
       " 'bhanwari',\n",
       " 'bhanwer',\n",
       " 'bhanweri',\n",
       " 'bharat',\n",
       " 'bharati',\n",
       " 'bharatlal',\n",
       " 'bharkah',\n",
       " 'bharkha',\n",
       " 'bharma',\n",
       " 'bhart',\n",
       " 'bhartendu',\n",
       " 'bharti',\n",
       " 'bhasker',\n",
       " 'bhatari',\n",
       " 'bhateri',\n",
       " 'bhatri',\n",
       " 'bhauk',\n",
       " 'bhavana',\n",
       " 'bhavesh',\n",
       " 'bhavisaya',\n",
       " 'bhavishy',\n",
       " 'bhavishya',\n",
       " 'bhavna',\n",
       " 'bhavuk',\n",
       " 'bhawan',\n",
       " 'bhawana',\n",
       " 'bhawani',\n",
       " 'bhawar',\n",
       " 'bhawari',\n",
       " 'bhawna',\n",
       " 'bheem',\n",
       " 'bheema',\n",
       " 'bhemji',\n",
       " 'bherav',\n",
       " 'bhhatu',\n",
       " 'bhikhari',\n",
       " 'bhiki',\n",
       " 'bhim',\n",
       " 'bhima',\n",
       " 'bhimsen',\n",
       " 'bhisan',\n",
       " 'bhismpal',\n",
       " 'bhiva',\n",
       " 'bhojaram',\n",
       " 'bhojpal',\n",
       " 'bhola',\n",
       " 'bholaram',\n",
       " 'bhole',\n",
       " 'bholi',\n",
       " 'bholu',\n",
       " 'bhonu',\n",
       " 'bhoop',\n",
       " 'bhoopsingh',\n",
       " 'bhoori',\n",
       " 'bhopal',\n",
       " 'bhorelal',\n",
       " 'bhotra',\n",
       " 'bhram',\n",
       " 'bhramanand',\n",
       " 'bhrat',\n",
       " 'bhrkat',\n",
       " 'bhteri',\n",
       " 'bhud',\n",
       " 'bhudev',\n",
       " 'bhudevi',\n",
       " 'bhudhashen',\n",
       " 'bhudhi',\n",
       " 'bhulaee',\n",
       " 'bhulan',\n",
       " 'bhuli',\n",
       " 'bhumika',\n",
       " 'bhundki',\n",
       " 'bhuneshwar',\n",
       " 'bhup',\n",
       " 'bhupen',\n",
       " 'bhupendar',\n",
       " 'bhupender',\n",
       " 'bhupendra',\n",
       " 'bhupesh',\n",
       " 'bhura',\n",
       " 'bhuralal',\n",
       " 'bhure',\n",
       " 'bhuri',\n",
       " 'bhusan',\n",
       " 'bhushan',\n",
       " 'bhuvneshwar',\n",
       " 'bhuwan',\n",
       " 'bidami',\n",
       " 'bidhya',\n",
       " 'bidur',\n",
       " 'bigan',\n",
       " 'biitu',\n",
       " 'bijali',\n",
       " 'bijander',\n",
       " 'bijender',\n",
       " 'bijendra',\n",
       " 'bikki',\n",
       " 'bikram',\n",
       " 'bilal',\n",
       " 'bilke',\n",
       " 'bilkis',\n",
       " 'bima',\n",
       " 'bimla',\n",
       " 'bimlesh',\n",
       " 'bina',\n",
       " 'binani',\n",
       " 'binda',\n",
       " 'binder',\n",
       " 'bindi',\n",
       " 'bindiya',\n",
       " 'bindu',\n",
       " 'bindvasini',\n",
       " 'binita',\n",
       " 'binja',\n",
       " 'binnu',\n",
       " 'binpal',\n",
       " 'bintu',\n",
       " 'binu',\n",
       " 'bipin',\n",
       " 'bipnesh',\n",
       " 'bir',\n",
       " 'biraj',\n",
       " 'birajbhushan',\n",
       " 'birajpal',\n",
       " 'biram',\n",
       " 'biran',\n",
       " 'birbal',\n",
       " 'birender',\n",
       " 'birgesh',\n",
       " 'birij',\n",
       " 'birjesh',\n",
       " 'birju',\n",
       " 'birma',\n",
       " 'birmadevi',\n",
       " 'birmati',\n",
       " 'bironica',\n",
       " 'birpal',\n",
       " 'birtha',\n",
       " 'biru',\n",
       " 'bishan',\n",
       " 'bishana',\n",
       " 'bishesh',\n",
       " 'bishnu',\n",
       " 'bishun',\n",
       " 'bismilla',\n",
       " 'bisto',\n",
       " 'bitoo',\n",
       " 'bittan',\n",
       " 'bitto',\n",
       " 'bittoo',\n",
       " 'bittu',\n",
       " 'bivla',\n",
       " 'biwa',\n",
       " 'bobbi',\n",
       " 'bobby',\n",
       " 'bobi',\n",
       " 'boby',\n",
       " 'bodh',\n",
       " 'bodu',\n",
       " 'boduram',\n",
       " 'bohati',\n",
       " 'braham',\n",
       " 'brahamprakash',\n",
       " 'brahmanand',\n",
       " 'braj',\n",
       " 'brajendra',\n",
       " 'bram',\n",
       " 'brij',\n",
       " 'brijesh',\n",
       " 'brijlal',\n",
       " 'brijmohan',\n",
       " 'brijnandan',\n",
       " 'brijpal',\n",
       " 'brijshwaer',\n",
       " 'brinda',\n",
       " 'brkha',\n",
       " 'bsram',\n",
       " 'buchhi',\n",
       " 'buddha',\n",
       " 'buddhi',\n",
       " 'budh',\n",
       " 'budhan',\n",
       " 'budhi',\n",
       " 'budho',\n",
       " 'budhram',\n",
       " 'budhu',\n",
       " 'budhwanti',\n",
       " 'buhan',\n",
       " 'buity',\n",
       " 'bulad',\n",
       " 'bulbul',\n",
       " 'bulet',\n",
       " 'bunda',\n",
       " 'bundel',\n",
       " 'bundhu',\n",
       " 'bunti',\n",
       " 'bunty',\n",
       " 'bupal',\n",
       " 'bushara',\n",
       " 'bushra',\n",
       " 'busness',\n",
       " 'busra',\n",
       " 'captain',\n",
       " 'chabi',\n",
       " 'chahat',\n",
       " 'chain',\n",
       " 'chajju',\n",
       " 'chakra',\n",
       " 'chama',\n",
       " 'chaman',\n",
       " 'chamanlal',\n",
       " 'chameli',\n",
       " 'champa',\n",
       " 'chanab',\n",
       " 'chanchal',\n",
       " 'chanchl',\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('Indian_Names.csv').read().splitlines()\n",
    "newords = [word.split(',')[1] for word in words]\n",
    "newords = newords[1:]\n",
    "words = newords\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '.': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {chr(i+ord('a')):i+1 for i in range(26)}\n",
    "stoi['.'] = 0\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos = {val: key for key,val in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.3752e-01,  2.6213e-03,  1.4740e+00,  1.4721e-01,  1.0053e-01,\n",
       "          1.2815e+00,  9.8663e-01,  5.0358e-01,  9.4115e-01, -5.0571e-01,\n",
       "         -1.7633e+00,  9.6312e-01, -5.0842e-01,  8.9683e-03,  4.4026e-01,\n",
       "          4.4172e-01],\n",
       "        [ 2.2739e+00, -8.5609e-01,  1.1246e+00,  8.1520e-01, -1.3253e+00,\n",
       "         -7.8403e-01, -6.3830e-01,  1.9174e+00, -6.3694e-01,  3.3515e-01,\n",
       "         -1.6344e+00,  5.2332e-01, -7.3500e-01, -1.0131e+00, -1.0928e-01,\n",
       "         -2.1181e-01],\n",
       "        [-2.8135e-01, -9.7059e-01,  1.9002e+00,  3.8136e-01,  9.5674e-01,\n",
       "          5.4711e-01, -1.6660e+00,  6.8776e-02, -5.0848e-01, -1.6454e+00,\n",
       "         -1.5790e+00,  2.4020e+00,  5.9122e-01, -1.1302e+00,  1.2713e-01,\n",
       "         -1.3631e-01],\n",
       "        [-7.5252e-02, -1.3489e+00, -5.1634e-01, -1.1411e+00,  6.3379e-01,\n",
       "          8.2426e-01,  1.3307e+00, -2.4644e-01, -1.1319e-01,  3.6333e-01,\n",
       "         -9.5452e-01,  7.3270e-01,  8.6277e-01,  1.2646e+00, -4.8542e-02,\n",
       "         -6.7197e-01],\n",
       "        [-4.4595e-01, -8.8615e-01,  8.1582e-01,  1.2555e+00, -1.1144e+00,\n",
       "          1.6190e+00,  1.8920e+00,  1.2949e+00, -1.7026e+00,  1.8266e+00,\n",
       "         -1.0766e+00, -1.9874e+00,  1.2762e+00,  1.0949e+00,  3.3330e-01,\n",
       "          7.1839e-01],\n",
       "        [-5.8257e-02,  2.0163e-01,  7.9373e-01, -1.6730e+00, -4.8095e-01,\n",
       "          1.1990e+00,  1.8931e+00, -1.0690e+00,  5.6292e-01, -1.6209e+00,\n",
       "          1.9029e-01, -3.4233e-01,  1.1095e+00,  1.3323e-01, -5.4811e-01,\n",
       "          3.0375e+00],\n",
       "        [-3.8830e-01, -2.3951e+00,  6.9679e-03,  9.4074e-02, -3.3012e-01,\n",
       "          2.4613e-01,  4.3330e-01,  1.8541e-01,  2.2792e-01, -2.2175e+00,\n",
       "         -5.7338e-01, -5.6932e-02, -1.8737e+00, -8.1889e-01,  1.2987e-01,\n",
       "          1.0274e+00],\n",
       "        [ 1.2586e+00, -3.7780e-01,  1.2735e+00,  8.1266e-02,  5.9279e-01,\n",
       "         -6.0236e-01, -2.6663e-01,  4.4394e-01,  9.3802e-02,  9.4555e-01,\n",
       "         -4.6882e-02,  1.4648e+00, -7.0047e-01,  1.3717e-01, -9.7627e-01,\n",
       "          9.7281e-01],\n",
       "        [-1.6932e-01, -1.0762e+00,  3.4382e-01, -5.0655e-02, -1.3472e+00,\n",
       "          1.0352e+00, -2.3116e+00,  2.0715e-01, -1.0579e+00,  1.0610e+00,\n",
       "          1.1256e+00,  1.2938e-02,  5.2180e-01,  4.2216e-01,  1.2684e+00,\n",
       "         -5.6814e-02],\n",
       "        [-3.7450e-01,  1.8076e+00,  1.8146e+00, -9.5879e-01, -8.0496e-01,\n",
       "          1.3172e+00,  2.5730e-01,  1.4879e+00,  4.3954e-01, -2.2211e+00,\n",
       "         -2.6130e-01,  5.9430e-01,  5.8984e-02, -3.4447e-01, -1.0416e-01,\n",
       "          3.4002e+00],\n",
       "        [-6.2717e-01, -1.4396e+00,  9.4470e-01, -3.7450e-01,  1.9446e-01,\n",
       "         -7.8961e-02, -8.2358e-01, -1.0658e+00,  6.1002e-01,  1.9239e+00,\n",
       "         -8.5587e-01,  1.3577e-01,  2.5620e+00,  9.2199e-01, -2.7322e-02,\n",
       "         -8.0583e-01],\n",
       "        [ 1.8666e+00,  1.0850e+00,  1.2206e+00,  1.5835e+00, -1.3241e+00,\n",
       "         -5.0259e-01, -7.9587e-01,  8.5657e-02, -2.7768e-01, -3.7823e-01,\n",
       "          7.7918e-01,  2.3653e-01,  7.7811e-01, -1.9555e+00,  1.0645e+00,\n",
       "          4.5328e-01],\n",
       "        [-6.0136e-01,  1.6943e+00, -1.4062e+00, -4.0174e-01,  1.2630e+00,\n",
       "          6.9637e-01,  1.3356e-01,  1.1493e+00, -4.1082e-01,  8.9971e-02,\n",
       "          8.3133e-01, -8.0574e-01,  7.1103e-01,  1.2952e+00,  3.8237e-01,\n",
       "         -2.0212e-01],\n",
       "        [ 2.5464e+00, -9.2779e-01,  1.5555e+00, -1.1973e+00,  2.8409e-01,\n",
       "         -1.2189e+00,  9.4044e-01, -1.0141e+00,  3.6951e-01, -8.4724e-01,\n",
       "          1.2069e+00,  1.0672e+00, -7.8008e-01,  1.2712e+00,  1.8956e+00,\n",
       "         -1.5892e+00],\n",
       "        [-4.4237e-01,  1.8205e-01, -1.7612e-01, -1.3338e+00,  5.8742e-01,\n",
       "         -4.0940e-01, -9.9128e-01,  1.4735e+00, -3.0243e-01, -4.2259e-01,\n",
       "          6.0568e-01,  4.7596e-01,  5.6179e-01,  5.3306e-01, -7.7281e-01,\n",
       "          1.1253e+00],\n",
       "        [ 3.1507e-01,  6.7013e-01, -1.9686e+00, -1.2719e+00, -1.8880e-01,\n",
       "          1.6339e+00, -5.3446e-01,  2.2939e+00, -1.8165e-01, -7.1978e-01,\n",
       "          1.0334e+00,  2.9989e-01,  3.9583e-01, -6.5348e-01, -6.9257e-01,\n",
       "          2.8861e-01],\n",
       "        [-5.1633e-01, -1.7866e+00,  6.7546e-01,  1.1752e-01, -5.9254e-01,\n",
       "          3.1702e-02,  4.3483e-01, -3.7615e-01,  8.4948e-01, -3.3497e-01,\n",
       "         -1.1348e+00, -4.9946e-01, -7.8532e-01,  1.3028e+00,  9.4752e-01,\n",
       "         -4.1113e-01],\n",
       "        [-1.0429e+00, -1.0267e+00, -7.9739e-01, -2.1278e-01, -6.9562e-01,\n",
       "         -1.0858e+00,  2.3668e+00,  2.8781e-01,  1.1717e+00,  1.6237e+00,\n",
       "         -1.3200e+00,  1.4982e+00, -1.8045e+00,  8.8935e-01,  6.6568e-01,\n",
       "          1.8958e+00],\n",
       "        [-1.0486e+00,  1.1471e+00, -6.3245e-01,  2.1428e-01, -3.3738e-01,\n",
       "         -1.0149e+00, -1.3019e-01,  1.7814e+00, -5.9831e-02,  4.8284e-01,\n",
       "          8.6697e-01,  3.3248e-01,  9.6515e-01, -8.1465e-01,  2.3612e-01,\n",
       "          2.3618e-01],\n",
       "        [ 1.6792e-01,  7.4327e-02,  5.2447e-01,  1.0264e+00, -1.0076e+00,\n",
       "         -1.3355e-01, -7.5131e-01,  9.3496e-02,  8.7151e-01,  8.6804e-01,\n",
       "          2.4453e-02,  2.7490e-01,  9.9891e-01, -7.1493e-01, -5.0638e-01,\n",
       "         -8.6219e-01],\n",
       "        [-1.9459e+00,  2.3529e+00,  1.0189e+00, -7.7713e-01, -4.1522e-01,\n",
       "         -5.5258e-01,  1.0662e+00, -1.3329e+00, -5.2001e-01,  6.7737e-01,\n",
       "         -4.9452e-01, -8.0051e-01,  1.0697e+00, -1.3710e-01,  7.4739e-01,\n",
       "         -1.2156e+00],\n",
       "        [ 1.3713e+00, -1.3316e+00, -2.6399e-01,  1.9975e+00,  1.0585e+00,\n",
       "          4.7258e-01,  1.5337e+00, -3.1847e-01,  2.7372e-01, -3.2880e-01,\n",
       "          5.6070e-01,  5.2890e-01,  4.2740e-01, -2.4668e+00,  1.4153e+00,\n",
       "         -9.5197e-01],\n",
       "        [-8.3210e-02, -3.8226e-01, -3.5745e-03,  1.4835e+00,  1.0070e+00,\n",
       "          2.6725e-01,  1.6419e+00,  8.5207e-01,  6.1749e-01,  9.1788e-02,\n",
       "         -3.1481e-01,  6.8344e-01,  6.8434e-02,  4.8224e-01, -2.3627e-01,\n",
       "          5.8295e-01],\n",
       "        [-1.8872e-01,  6.5347e-01,  1.8458e-01,  7.7286e-01, -6.1691e-01,\n",
       "         -3.4137e-01,  1.4249e+00,  1.5899e-01,  5.8883e-01, -5.1439e-01,\n",
       "          1.7858e+00, -6.2842e-01, -2.0669e+00,  2.3833e+00,  2.1584e-01,\n",
       "         -2.4281e+00],\n",
       "        [ 2.5109e+00,  3.9256e-01,  8.6458e-01, -3.1703e-01,  3.6192e-01,\n",
       "          1.8763e-01,  1.9743e-01,  4.4055e-02,  7.2455e-01,  1.5548e+00,\n",
       "          8.7272e-01,  2.1407e+00,  8.7230e-01,  1.4955e+00,  1.3447e+00,\n",
       "          2.2045e-01],\n",
       "        [ 8.8001e-01,  1.2886e+00, -1.1958e+00,  8.6575e-01, -1.1510e+00,\n",
       "         -8.9490e-01,  7.3959e-01,  2.1736e+00, -7.6131e-01,  1.9564e+00,\n",
       "          7.4000e-01, -1.4250e+00,  4.4221e-01, -1.9156e+00,  4.0328e-01,\n",
       "          6.3692e-01],\n",
       "        [ 9.8232e-01, -3.7921e-01, -3.6503e-01,  1.5516e-01, -2.5398e-01,\n",
       "          5.4144e-02, -1.3400e-01,  8.8090e-02,  5.6838e-01, -6.4301e-01,\n",
       "         -7.8174e-01, -3.7133e-01, -7.0003e-01,  6.0837e-02,  1.3952e+00,\n",
       "         -3.4785e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.randn((27,16))\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 6\n",
    "X, Y = [], []\n",
    "# print(stoi)\n",
    "for w in words:\n",
    "  if(sum(stoi.get(ch,-1)==-1 for ch in w)!=0):\n",
    "    continue\n",
    "  context = [0] * block_size\n",
    "  for ch in w + '.':\n",
    "    ix = stoi[ch]\n",
    "    X.append(context)\n",
    "    Y.append(ix)\n",
    "    #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "    context = context[1:] + [ix] # crop and append\n",
    "  \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38043, 6]) torch.Size([38043])\n",
      "torch.Size([4784, 6]) torch.Size([4784])\n",
      "torch.Size([4738, 6]) torch.Size([4738])\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  for w in words:\n",
    "    if(sum(stoi.get(ch,-1)==-1 for ch in w)!=0):\n",
    "        continue\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([47565, 6, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncnt = [96,100,27]\n",
    "g = torch.Generator().manual_seed(123)\n",
    "#weights and biases\n",
    "w1 = torch.randn((ncnt[0],ncnt[1]),generator=g) * (5/3) / (ncnt[0]*ncnt[1])**0.5\n",
    "# b1 = torch.randn(ncnt[1],generator=g) * 0.01\n",
    "w2 = torch.randn((ncnt[1],ncnt[2]),generator=g) * 0.01\n",
    "b2 = torch.randn(ncnt[2],generator=g) * 0\n",
    "\n",
    "#batchnorm parameters\n",
    "bngain = torch.ones((1,ncnt[1]))\n",
    "bnbias = torch.zeros((1,ncnt[1]))\n",
    "bnmean_running = torch.zeros((1,ncnt[1]))\n",
    "bnstd_running = torch.ones((1,ncnt[1]))\n",
    "\n",
    "\n",
    "parameters = [C,w1,w2,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 100])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12759"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/20000: 3.2993\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for i in range(20000):\n",
    "    #getting the batch\n",
    "    ix = torch.randint(0,Xtr.shape[0],(32,))\n",
    "\n",
    "\n",
    "    #getting the vector which represents them\n",
    "    emb = C[Xtr[ix]]\n",
    "\n",
    "\n",
    "    #linear layer\n",
    "    preh = emb.view(-1,ncnt[0]) @ w1\n",
    "\n",
    "    #batchnorm layer\n",
    "    bnmeani = preh.mean(0,keepdim=True)\n",
    "    bnstdi = preh.std(0,keepdim=True)\n",
    "    preh = bngain * (preh-bnmeani)/bnstdi + bnbias\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "\n",
    "\n",
    "\n",
    "    #non linearity\n",
    "    h = torch.tanh(preh)\n",
    "    logits = h @ w2 + b2\n",
    "    loss = F.cross_entropy(logits,Ytr[ix])\n",
    "\n",
    "\n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    #learning\n",
    "    lr = 0.002 if i<10000 else 0.001\n",
    "    for p in parameters:\n",
    "        p.data += -lr*p.grad\n",
    "    if(i%10000==0):\n",
    "        print(f'{i:7d}/20000: {loss.item():.4f}')\n",
    "    losses.append(loss.log10().item())\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ddb0f24170>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    emb = C[Xtr] \n",
    "    preh = emb.view(-1, ncnt[0]) @ w1 \n",
    "    bnmean = preh.mean(0,keepdim=True)\n",
    "    bnstd = preh.std(0,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 3.2938380241394043\n",
      "val 3.2937850952148438\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() \n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x]\n",
    "  embcat = emb.view(emb.shape[0], -1) \n",
    "  hpreact = embcat @ w1 \n",
    "  #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n",
    "  hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "  h = torch.tanh(hpreact) \n",
    "  logits = h @ w2 + b2 \n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 16])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = [0] * block_size\n",
    "C[torch.tensor([context])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gjfiwutvcrmbe\n",
      "aynaajzxzppwhgodwebupgdetpfv\n",
      "n\n",
      "xpxwrjyhdpjuqtjxhuqovhlypirvcgowiblnbibsvavoctnpntwr\n",
      "nmrfhetukiirnnpyagicubhbnuhcypibqpdjhkdaksrwpfvzvm\n",
      "czhtoawyuvlbfzkepwqphrssvjgkx\n",
      "mjpxxf\n",
      "caadgmmrmjmwrjswzzsekxgrviqokndihtsorhmcesovundzeyhrzopccwtfgbvelgowjohdbqkvdao\n",
      "agexyvrgxgnysjdttpigggvmxyykdezetdjcgmxwjgffsrnlctapzsdmxdmjgjdtarkh\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    \n",
    "    out = []\n",
    "    context = [0]*block_size\n",
    "    while True:\n",
    "      emb = C[torch.tensor([context])] \n",
    "      preh = emb.view(1,-1) @ w1  \n",
    "      preh = bngain * (preh - bnmean_running) / bnstd_running + bnbias\n",
    "      h = torch.tanh(preh)\n",
    "      logits = h @ w2 + b2\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out)[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187086\n"
     ]
    }
   ],
   "source": [
    "class Linear:\n",
    "    def __init__(self,fan_in,fan_out,bias=True):\n",
    "        self.weight = torch.randn((fan_in,fan_out)) / fan_in**0.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight \n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight]+([self.bias] if self.bias is not None else [])\n",
    "\n",
    "class BatchNorm1D:\n",
    "\n",
    "    def __init__(self,dim,eps=1e-5,momentum = 0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        if self.training:\n",
    "            xmean = x.mean(0,keepdim=True)\n",
    "            xvar = x.var(0,keepdim=True)\n",
    "        else: \n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        xout = (x - xmean) / torch.sqrt(xvar+self.eps)\n",
    "        self.out = self.gamma * xout + self.beta\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1-self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1-self.momentum) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma,self.beta]\n",
    "\n",
    "class Tanh:\n",
    "\n",
    "    def __call__(self,x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class Embedding:\n",
    "    def __init__(self,vocab_size,dim):\n",
    "        self.weight = torch.randn((vocab_size,dim))\n",
    "\n",
    "    def __call__(self,ix):\n",
    "        return self.weight[ix]\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "\n",
    "class Flatten:\n",
    "    def __call__(self,x):\n",
    "        return x.view(x.shape[0],-1)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class Sequential:\n",
    "    def __init__(self,layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [par for layer in self.layers for par in layer.parameters()]\n",
    "    \n",
    "    \n",
    "ncnt = [96,200,27]\n",
    "C = torch.randn((27,16))\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(27,16),\n",
    "    Flatten(),\n",
    "    Linear(ncnt[0],ncnt[1],False),BatchNorm1D(ncnt[1]),Tanh(),\n",
    "    Linear(ncnt[1],ncnt[1],False),BatchNorm1D(ncnt[1]),Tanh(),\n",
    "    Linear(ncnt[1],ncnt[1],False),BatchNorm1D(ncnt[1]),Tanh(),\n",
    "    Linear(ncnt[1],ncnt[1],False),BatchNorm1D(ncnt[1]),Tanh(),\n",
    "    Linear(ncnt[1],ncnt[1],False),BatchNorm1D(ncnt[1]),Tanh(),\n",
    "    Linear(ncnt[1],ncnt[2],False),BatchNorm1D(27)\n",
    "])\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.layers[-1].gamma *= 0.1\n",
    "    for layer in model.layers[:-1]:\n",
    "        if isinstance(layer,Linear):\n",
    "            layer.weight *= 5/3\n",
    "\n",
    "parameters = model.parameters()\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/200000: 3.2920\n",
      "   1000/200000: 2.6224\n",
      "   2000/200000: 2.2404\n",
      "   3000/200000: 2.0179\n",
      "   4000/200000: 2.1481\n",
      "   5000/200000: 1.9436\n",
      "   6000/200000: 1.9709\n",
      "   7000/200000: 2.1084\n",
      "   8000/200000: 2.1844\n",
      "   9000/200000: 1.6719\n",
      "  10000/200000: 1.7752\n",
      "  11000/200000: 2.0399\n",
      "  12000/200000: 1.5061\n",
      "  13000/200000: 1.7744\n",
      "  14000/200000: 1.8139\n",
      "  15000/200000: 1.7283\n",
      "  16000/200000: 1.8229\n",
      "  17000/200000: 1.7611\n",
      "  18000/200000: 1.8348\n",
      "  19000/200000: 1.8548\n"
     ]
    }
   ],
   "source": [
    "#lets go to the training \n",
    "losses = []\n",
    "batch_size = 32\n",
    "for i in range(20000):\n",
    "    #making batch\n",
    "    ix = torch.randint(0,Xtr.shape[0],(batch_size,))\n",
    "    Xb, Yb = Xtr[ix],Ytr[ix]\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits,Yb)\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    lr = 0.1 if i<10000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr*p.grad\n",
    "\n",
    "    if i%1000==0:\n",
    "        print(f'{i:7d}/200000: {loss.item():.4f}')\n",
    "    losses.append(loss.log10().item())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1.608314871788025\n",
      "val 2.077179431915283\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  logits = model(x)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "for layer in model.layers:\n",
    "  layer.training = False\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rishi\n",
      "shaman\n",
      "gurvon\n",
      "devukhagri\n",
      "veero\n",
      "kajal\n",
      "renesh\n",
      "sandha\n",
      "jinaa\n",
      "sasnau\n",
      "sattap\n",
      "baidsni\n",
      "kuntil\n",
      "rhishur\n",
      "sanqha\n",
      "kanchima\n",
      "sarodha\n",
      "dijsaphl\n",
      "kalki\n",
      "siyaram\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    \n",
    "  out = []\n",
    "  context = [0] * block_size \n",
    "  while True:\n",
    "    # forward pass\n",
    "    logits = model(torch.tensor([context])) \n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "    context = context[1:] + [ix]\n",
    "    out.append(ix)\n",
    "    if ix == 0:\n",
    "      break\n",
    "        \n",
    "  print(''.join(itos[i] for i in out[:-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
